[{"authors":["admin"],"categories":null,"content":"Jérémie Deray is a software engineer at Canonical, the company publishing Ubuntu . As part of the robotics team he is working on both ROS and ROS 2 - development, maintenance, dev tools, security, snaps\u0026hellip; Before that he worked five years at PAL Robotics on navigation, perception and a little bit of control, this for various robots - mobile-base, mobile-manipulator, semi-humanoid.\nJérémie is also a PhD in the field of Simultaneous Localization and Mapping (SLAM) applied to industrial mobile-bases. The research has taken place in a collaborative framework between the company PAL Robotics and the Universitat Politècnica De Catalunya (IRI-UPC), both in the lovely city of Barcelona, Spain. The research work has been supervised by Joan Solà and Juan Andrade Cetto.\n","date":-62135596800,"expirydate":-62135596800,"kind":"taxonomy","lang":"en","lastmod":1604870324,"objectID":"2525497d367e79493fd32b198b28f040","permalink":"https://artivis.github.io/author/jeremie-deray/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/author/jeremie-deray/","section":"authors","summary":"Jérémie Deray is a software engineer at Canonical, the company publishing Ubuntu . As part of the robotics team he is working on both ROS and ROS 2 - development, maintenance, dev tools, security, snaps\u0026hellip; Before that he worked five years at PAL Robotics on navigation, perception and a little bit of control, this for various robots - mobile-base, mobile-manipulator, semi-humanoid.","tags":null,"title":"Jérémie Deray","type":"authors"},{"authors":null,"categories":null,"content":"In this post we will see how the Pi Pico can natively speak to a ROS2 graph using micro-ROS. We will set up a project in VSCode, compile and upload it to the microcontroller. We thus assume that you are somewhat familiar with ROS2 development and VSCode.\n Content   What is this all about?   The Raspberry Pi Pico  micro-ROS    Getting started   Installing dependencies  Fetching the sources  Setting up VSCode    Running the example   Wait a minute. What does it do?  Uploading to the Pi Pico  Installing the micro-ros-agent  Actually running the example    What\u0026rsquo;s next?   What is this all about? The Raspberry Pi Pico The Raspberry Pi Pico, announced in late January 2021, is the newest release of the Raspberry Pi Foundation which received a ton of attention (a quick search on Google and/or Youtube will convince you). And that\u0026rsquo;s for a good reason. Compared to its well known predecessors, this new board differs in two major ways: it is an in-house designed open-hardware microcontroller! Yes, the chip itself is designed by the Pi\u0026rsquo;s engineers and it is fully open-hardware. And as usually with the Pi foundation, it is incredibly affordable at just 4$.\nThe details concerning the board itself, the differences between microprocessor and microcontroller, the 101 getting started or what can the Pi Pico do; all of that is beyond the scope of this post. But I strongly encourage you having a look for yourself, whether you are familiar with microcontrollers or not.\nmicro-ROS In the ROS (1) realm, microcontrollers have always been sort of second class citizens. They can\u0026rsquo;t interact directly with the ROS graph and developers have to rely on libraries such as rosserial. But ROS2 is a whole new world and things are changing.\n micro-ROS puts ROS 2 onto microcontrollers, making them first class participants of the ROS 2 environment.\n The micro-ROS project is an effort led by big industrial names such as Bosch, eProsima, Fiware Foundation, notably through the OFERA H2020 project, and a myriad of partners and collaborators including e.g. Amazon and Canonical.\nSo what is it? It is essentially a thin wrapper (see its design document) on top of \u0026lsquo;DDS for eXtremely Resource Constrained Environments\u0026rsquo; ( DDS-XRCE), running on a real-time OS, allowing microcontrollers to \u0026lsquo;speak\u0026rsquo; to a ROS2 graph (the usual talker/listener) using an optimized subset of the DDS protocol. It relies on a \u0026lsquo;bridged\u0026rsquo; communication architecture with a \u0026lsquo;broker\u0026rsquo; named the \u0026lsquo;micro-ros-agent\u0026rsquo;. The agent is in charge of the interfacing between the ROS2 graph and one or several micro-ROS devices.\nMore details can be found on the micro-ROS website including how it compares/differs from rosserial (see here and here).\nGetting started Alright, so now that we have clarified a couple terms, let us get started, step by step, with micro-ROS on Pi Pico with the official example available on github. Note that for this tutorial I am running Ubuntu 20.04 with the VSCode snap.\nIf you are not running Ubuntu 20.04 yet, you could consider using a LXD container. You can refer to my previous post \u0026lsquo;ROS Noetic development workflow in LXC\u0026rsquo; to help you get started setting up the container.\nInstalling dependencies Let\u0026rsquo;s start simple by installing the couple necessary dependencies,\nsudo apt install build-essential cmake gcc-arm-none-eabi libnewlib-arm-none-eabi doxygen git python3  Fetching the sources We will now create a workspace and fetch all the sources,\nmkdir -p ~/micro_ros_ws/src cd ~/micro_ros_ws/src git clone --recurse-submodules https://github.com/raspberrypi/pico-sdk.git git clone https://github.com/micro-ROS/micro_ros_raspberrypi_pico_sdk.git  The first repository is the Pi Pico SDK provided by the Pi foundation. The second contains a precompiled micro-ROS stack together with a hello-world-like example.\nSetting up VSCode Let us now open the example in VSCode and set it up. To follow along, you will need two VSCode extensions that are rather common for C++ development. These extensions are the C++ extension and CMake tools for VSCode. After installing them, we will create a configuration file for CMake tools and set a variable so that our project knows where to find the Pi Pico SDK. To do so, simply type,\ncd ~/micro_ros_ws/src/micro_ros_raspberrypi_pico_sdk mkdir .vscode touch .vscode/settings.json  Open the newly created file with your favorite editor,\nvi .vscode/settings.json  and add the following,\n{ \u0026quot;cmake.configureEnvironment\u0026quot;: { \u0026quot;PICO_SDK_PATH\u0026quot;: \u0026quot;/home/artivis/micro_ros_ws/src/pico-sdk\u0026quot;, }, }  This variable is an environment variable that is only passed to CMake at configuration time. See the CMake-Tools documentation for more info.\nLet us now open it,\ncode .  Before running the CMake configuration and build it, we must select the appropriate \u0026lsquo;kit\u0026rsquo; (maybe VSCode has already asked you to do so). Open the palette (ctrl+shift+p) and search for 'CMake: Scan for Kits' and then 'CMake: Select a Kit' and make sure to select the compiler we\u0026rsquo;ve installed above, that is 'GCC for arm-non-eabi'.\nWe\u0026rsquo;re all set, let us build the example! Open the palette again and hit 'CMake: Build'.\nRunning the example Wait a minute. What does it do? Right, let\u0026rsquo;s break down very briefly what the example does. It sets up a node called 'pico_node', then a publisher publishing a 'std_msgs/msg/int32.h' message on topic 'pico_publisher', a recurring timer and an executor to orchestrate everything. Every 0.1 second, the executor spins. But only every second, the timer will have the publisher publish a message and increase the message data by 1. Simple. So let\u0026rsquo;s try it out.\nUploading to the Pi Pico If everything went fine during compilation, you should see a new 'build' folder in your project view. In this folder, you will find the file that we should now upload to the Pi Pico, it is named here 'pico_micro_ros_example.uf2'. To upload it, simply connect the board with a USB cable while pressing the tiny white button labelled 'BOOTSEL'. Doing so, the Pi Pico will mount similarly to a flash drive allowing us to very easily copy/paste the \u0026lsquo;.uf2\u0026rsquo; file.\nHead to a terminal and type,\ncd build cp pico_micro_ros_example.uf2 /media/$USER/RPI-RP2  Once the file is copied, the board will automatically reboot and start executing the example.\nEasy-peasy.\nInstalling the micro-ros-agent We have seen in the introduction that micro-ROS has a bridged communication architecture. We thus have to build that bridge. Well, fortunately the development team has built it already and distributes it both as a Snap or a Docker image. Here we\u0026rsquo;ll make use of the former. If you are using Ubuntu 16.04 or later, snap is already pre-installed and ready to go. If you are running another OS, you can either install snap or make use of the Docker image.\nTo install the micro-ros-agent snap, type,\nsudo snap install micro-ros-agent  After installing it, and because we are using a serial connection, we need to configure a couple things. First we need to enable the 'hotplug' feature,\nsudo snap set core experimental.hotplug=true  and restart the snap demon so that it takes effect,\nsudo systemctl restart snapd  After making sure the Pi Pico is plugged, execute,\n$ snap interface serial-port name: serial-port summary: allows accessing a specific serial port plugs: - micro-ros-agent slots: - snapd:pico (allows accessing a specific serial port)  What we see here is that the micro-ros-agent snap has a serial \u0026lsquo;plug\u0026rsquo; while a \u0026lsquo;pico\u0026rsquo; 'slot' magically appeared. As per the semantic, we probably should connect them together. To do so run,\nsnap connect micro-ros-agent:serial-port snapd:pico  We are now all set to finally run our example.\nActually running the example With the Pi Pico plugged through USB, we will start the micro-ros-agent as follows,\nmicro-ros-agent serial --dev /dev/ttyACM0 baudrate=115200  and wait a couple seconds for the Pi Pico\u0026rsquo;s LED to light up indicating that the main loop is running. In case it does not light up after a few long seconds (count up to 10 mississippi), you may want to unplug/replug the board in order to reboot it. The initialization procedure of the example lacks a few error checking. Hey, could fixing that be your first project?\nSo now the LED should shine a bright green. That\u0026rsquo;s cool. Do you know what\u0026rsquo;s cooler? Running on your host machine,\n$ source /opt/ros/dashing/setup.bash $ ros2 topic echo /pico_publisher data: 41 --- data: 42 ---  Awesome!\nAnd hitting a\n$ ros2 node list /pico_node  proves that the micro-ROS node running on the Pi Pico is visible to ROS2 on the host machine.\nYatta!\nWhat\u0026rsquo;s next? For a long time it wasn\u0026rsquo;t convenient to mix microcontrollers and ROS. But this is about to seriously change as we\u0026rsquo;ve just seen. No doubt that both micro-ROS and the Pi Pico will bolster great robotics applications (and more!).\nIn this tutorial we\u0026rsquo;ve reached a great starting point with a ROS2-based project ready to spin on the suppa-cool suppa-affordable Pi Pico.\nOf course this wouldn\u0026rsquo;t have been possible without the micro-ROS dev team and Cyberbotics engineer Darko Lukić ( @lukicdarkoo) who has put together the initial example we\u0026rsquo;ve just used. As often, there are super smart people out there making complicated stuff very accessible, shout out to them.\nI\u0026rsquo;m personally going to keep playing with micro-ROS on Pi Pico, first because it is fun and second because I have a couple ideas up my sleeves. Be sure that if they become reality you\u0026rsquo;ll hear about them on this blog.\nWhat about you? Do you have some cool projects already in mind?\n","date":1615420800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1615599637,"objectID":"4d1951342225bc7e104924acfe1b4e7c","permalink":"https://artivis.github.io/post/2021/pi-pico-uros-getting-started/","publishdate":"2021-03-11T00:00:00Z","relpermalink":"/post/2021/pi-pico-uros-getting-started/","section":"post","summary":"In this post we will see how the Pi Pico can natively speak to a ROS2 graph using micro-ROS. We will set up a project in VSCode, compile and upload it to the microcontroller.","tags":["tutorial","Raspberry Pi","Pi Pico","ROS2","micro-ROS","VSCode"],"title":"Getting started with micro-ROS on the Pi Pico","type":"post"},{"authors":null,"categories":null,"content":"In this post we will see how to wire up an HC-SR04 range sensor to the Raspberry Pi Pico and publish its readings to the ROS 2 graph using micro-ROS. This builds upon the previous post \u0026lsquo;Getting started with micro-ROS on the Pi Pico\u0026rsquo;, as such I\u0026rsquo;d encourage you to read it first if you are not already familiar with the topic.\n Content   A sensor you said?   \u0026ldquo;I\u0026rsquo;m Bat(sensor)\u0026quot;  Which sonar model to pick?    Wiring up the sonar to the Pi Pico   Micro code for micro-ROS  Bip bip bip    What\u0026rsquo;s next?   A sensor you said? In this tutorial, we will make use of one of the most common sensors in robotics, a sonar. These sensors are cheap, fairly simple to use and surprisingly precise under favorable circumstances. Sonars are used to measure distances and can therefore be used to detect and locate obstacles so that one can make sure its robot does not run into nearby things. It is no mystery why they are so incredibly popular.\nSo, what\u0026rsquo;s a sonar, how does it work, which one should I pick?\n\u0026ldquo;I\u0026rsquo;m Bat(sensor)\u0026rdquo; A sonar is an echolocation sensor which allows for measuring distances. To keep things simple to digest, sonars work the same way as bats do. Nah they don\u0026rsquo;t fly, hunting bugs at dawn; nor do they fight crime. I obviously meant the same way as bats perceive the world. A sonar is composed of an emitter and a receiver. The former emits an ultrasound signal, which will bounce off of facing obstacles, back at the receiver. Measuring the time difference between the signal emission and reception, one can easily calculate the distance travelled by the sound wave and thus the distance to the obstacle that reverberated the sound.\nThis technology can suffer from all kind of issues if, for example, the sound wave is reverberated away from the sensor, or if it is reverberated back to the sensor by two obstacles at different distances, or if it is absorbed by some sound dampening material.\nIf this is still unclear or you want to know more about it, I\u0026rsquo;ll redirect you to your favorite web search engine as it is a little out of the scope of this post.\nWhich sonar model to pick? There exists plenty different models of sonars, fortunately most of them works the same way. Furthermore they can be found for a few bucks per unit on the internet. Head on over to your favorite electronics supplier and you\u0026rsquo;re sure to find them for sale.\nI\u0026rsquo;d recommend you use the \u0026lsquo;HC-SR04P\u0026rsquo; model which is the 3.3V variant of the immensely popular \u0026lsquo;HC-SR04\u0026rsquo; (5V). It is this model (the \u0026lsquo;HC-SR04P\u0026rsquo;) I will be using in this post. Given its very small power consumption, it can be powered up directly from the Pi Pico and thus the whole setup can simply be powered from the USB cable. In case you are using a 5V package, make sure to adapt the wiring described below or else you are risking damages to your Pi Pico! For reference, I ordered a lot of 5 units for less than 10$ including shipping. Including the Pi Pico price, that\u0026rsquo;s a lot of sensing for the price!\nWiring up the sonar to the Pi Pico The \u0026lsquo;HC-SR04P\u0026rsquo; board comes with four pins labelled \u0026lsquo;Vcc\u0026rsquo;, \u0026lsquo;Gnd\u0026rsquo;, \u0026lsquo;Trig\u0026rsquo; and \u0026lsquo;Echo\u0026rsquo;. As you\u0026rsquo;ve already guessed, the Vcc and Gnd pins are for the 3.3V line and the ground respectively, while \u0026lsquo;Trig\u0026rsquo; is used to trigger the sensor and \u0026lsquo;Echo\u0026rsquo; reports the reception of the echo (the reception of the sound wave that bounced off of an obstacle).\nFrom there, and referring to the Pi Pico pinout, the wiring is straight forward:\n connect Pico\u0026rsquo;s pin 36 (3V3 OUT) to the sonar\u0026rsquo;s \u0026lsquo;Vcc\u0026rsquo; connect Pico\u0026rsquo;s pin 38 (GND) to the sonar\u0026rsquo;s \u0026lsquo;Gnd\u0026rsquo; connect Pico\u0026rsquo;s pin 9 (GPIO 6) to the sonar\u0026rsquo;s \u0026lsquo;Echo\u0026rsquo; connect Pico\u0026rsquo;s pin 10 (GPIO 7) to the sonar\u0026rsquo;s \u0026lsquo;Trig\u0026rsquo;  This setup is depicted in the figure below.\nAs far as the hardware goes, we\u0026rsquo;re done. Let us move to the software.\nMicro code for micro-ROS We\u0026rsquo;ve seen in the \u0026lsquo;previous post\u0026rsquo; how to set up VSCode for programming micro-ros, and how to compile and flash a program on the Pi Pico. We\u0026rsquo;ll thus refer to the aforementioned post on how to do all that and skip it here. Similarly, we\u0026rsquo;ll skip all the boilerplate code and only show the bits specific to our application. However, know that this example (and more) is fully available on github at artivis/mico_ros.\nAlright, let\u0026rsquo;s dive a little. Our application is essentially composed of two functions, one that triggers and reads the sensor, and a second, the timer callback, which calls the first, fill up a ROS message and publishes it:\n... // The GPIO pins to which the sonar is wired #define GPIO_ECHO 6 #define GPIO_TRIGGER 7 /** * @brief Get the range value in meter. */ float read_range() { // Send an impulse trigger of 10us gpio_put(GPIO_TRIGGER, 1); sleep_us(10); gpio_put(GPIO_TRIGGER, 0); // Read how long is the echo uint32_t signaloff, signalon; do { signaloff = time_us_32(); } while (gpio_get(GPIO_ECHO) == 0); do { signalon = time_us_32(); } while (gpio_get(GPIO_ECHO) == 1); // Actual echo duration in us const float dt = signalon - signaloff; // distance in meter: // echo duration (us) x speed of sound (m/us) / 2 (round trip) return dt * 0.000343 / 2.0; } ... /** * @brief Read the range from the sensor, * fill up the ROS message and publish it. */ void timer_callback(rcl_timer_t *timer, int64_t /*last_call_time*/) { if (timer) { range_msg.range = read_range(); fill_msg_stamp(range_msg.header.stamp); rcl_publish(\u0026amp;publisher, \u0026amp;range_msg, NULL); } else { printf(\u0026quot;Failed to publish range. Continuing.\\n\u0026quot;); } } ...  That\u0026rsquo;s pretty much it. The rest of the code is mostly boilerplate, initializing the GPIO, setting up the micro-ROS node, publisher, timer and executor, and having it all spin.\nNote that we are using the standard sensor_msgs/msgs/Range message. You can find its definition and a breakdown of its field online in the ROS2 API documentation.\nAll there is to do now is to compile the code, flash the resulting \u0026lsquo;.uf2\u0026rsquo; file and start the micro-ROS agent.\nBip bip bip Assuming compiling and flashing went all fine, all we have to do is to plug the board to our computer and launch the micro-ROS agent. We do so with the following command:\ndocker run -it --rm -v /dev:/dev --privileged --net=host microros/micro-ros-agent:foxy serial --dev /dev/ttyACM0 -b 115200  Let\u0026rsquo;s see if we get anything,\n$ ros2 topic list /parameter_events /pico/range /rosout  the topic /pico/range is advertised, that\u0026rsquo;s a good start. Let see what it contains,\n$ ros2 topic echo /pico/range header: stamp: sec: 145 nanosec: 837599000 frame_id: pico_sonar_0_link radiation_type: 0 field_of_view: 30.0 min_range: 0.019999999552965164 max_range: 4.0 range: 12.138598442077637 --- header: stamp: sec: 145 nanosec: 915356000 frame_id: pico_sonar_0_link radiation_type: 0 field_of_view: 30.0 min_range: 0.019999999552965164 max_range: 4.0 range: 12.138941764831543 --- ...  That looks great!\nYou could now play with your new sensor, moving an obstacle back and forth in front of it. Take a measuring tape and compare the reported distance to the measured one, you may be surprised by its accuracy, I know I was.\nWhat\u0026rsquo;s next? This is a neat little project to approach micro-ROS and the possibilities it opens. Indeed it is really rewarding to see the actual distance between the sensor and an obstacle being readily available on our ROS 2 graph. But taking a step back we can start seeing an slightly larger picture; a picture in which one will be able to easily, effortlessly, add plug\u0026rsquo;n\u0026rsquo;play ROS2-ready hardware modules to existing robots. Plug a camera and its feed magically appears on the graph, an IMU module could provide a reliable odometry source, a motor ready to spin in a snap (pun intended).\nWe\u0026rsquo;re not there yet, but I\u0026rsquo;m definitely going to follow this line of thought for my own Turtlebot 3. Can you believe that it doesn\u0026rsquo;t have any sonar?!\nFortunately now I can easily add a pair of them and have my robot stop bumping into my lazy cat lying down on the floor, unwitting ninja, invisible to the laser scanner 😅.\n","date":1615420800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1621458853,"objectID":"0888a684eacd5d63bd108ef62df75f84","permalink":"https://artivis.github.io/post/2021/pi-pico-uros-sonar/","publishdate":"2021-03-11T00:00:00Z","relpermalink":"/post/2021/pi-pico-uros-sonar/","section":"post","summary":"In this post we will see how to wire up an HC-SR04 range sensor to the Raspberry Pi Pico and publish its readings to the ROS 2 graph using micro-ROS. This builds upon the previous post \u0026lsquo;Getting started with micro-ROS on the Pi Pico\u0026rsquo;, as such I\u0026rsquo;d encourage you to read it first if you are not already familiar with the topic.","tags":["tutorial","Raspberry Pi","Pi Pico","ROS2","micro-ROS","sensors"],"title":"Publishing sonar readings with micro-ROS on the Raspberry Pi Pico","type":"post"},{"authors":null,"categories":null,"content":"In this post we will discover the great magazines edited under the Raspberry Pi Press umbrella and discuss how to easily access them all.\n Content   The Raspberry Pi Press magazines  Bookshelf  rpipress-downloader   The Raspberry Pi Press magazines The Raspberry Pi Press is a part of the Raspberry Pi Foundation and the publisher of a great deal of magazines and books. Among the many magazines edited, some are freely available for download,\n  HackSpace is a monthly publication dedicated to those who love to make things and learn while doing it.  HelloWorld is published three times a year and targets educators of the computing and digital world.  MagPi is the official magazine of the Raspberry Pi and is loaded with stories and project based on the single board computer. Published every month, the latest issue is numbered N°95 as of the time of writing, making it an incredible source of inspiration.  Wiredframe is published every 2 weeks and is entirely dedicated to video games. But unlike other video game magazines, it offers to look at how they are made, who make them and offer a lot of resources to get started writing your own games.  On top of that, The Raspberry Pi Press also publishes many great books.\nEach magazine can be bought online and shipped around the globe. One can also sign for a yearly subscription, offering some discount and/or goodies. At the same time, issues are freely available to download in pdf from the magazine websites.\nBookshelf Recently the Raspberry Pi Foundation presented the Raspberry Pi OS, a rebranding of Raspbian, highlighting some of its novelties. Among those novelties, they showcased a neat little app named Bookshelf that allows you to browse and download the issues of several magazines edited by the Raspberry Pi Press.\nThe application is a simple interface listing all issues of each magazine but also some of the books. It allows for simply downloading any issue by simply clicking on the desired cover.\nUnfortunately this great app is only available through the Pi OS archive and its source code is not public at the time of writing. One can still download the deb package and install it manually. To do so, visit the app archive and look for the latest version of the debian package for your machine architecture. At the moment it is rp-bookshelf_0.4_amd64.deb for common computers. From there, we can simply download the debian and install it,\n$ wget http://archive.raspberrypi.org/debian/pool/main/r/rp-bookshelf/rp-bookshelf_0.4_amd64.deb $ dpkg -i rp-bookshelf_0.4_amd64.deb  To launch the app simply type,\n$ rp-bookshelf  Altho this procedure works fine, it is a little unpleasant. Furthermore, I personally don\u0026rsquo;t care much about the GUI and I\u0026rsquo;d rather prefer to automatically download the latest issues I care for. If you feel the same, keep on reading.\nrpipress-downloader The Raspberry Pi Press Store was recently entirely redesigned bringing some uniformization across all the magazine websites. That allows us to write a small web scrapping script to automatically download the latest (or all) issues and books of our favorite magazine(s).\nSo I went ahead and did just that, writing a small Python script that you can find on Github, or conveniently install as a Snap as follows,\nsudo snap install rpipress-downloader  Its use it pretty simple, launch the script in a terminal and by default it will automatically search and download the latest issue of all aforementioned magazines.\nFurther options let you:\n specify which magazine to download rpipress-downloader --magazines magpi hackspace   download all issues, rpipress-downloader --all   download the books too, rpipress-downloader --books   combine options so that, rpipress-downloader -a -m magpi -b  will download all MagPi issues and books.\n  Issues and books are saved respectively in\n ~/rpipress/{magazine} ~/rpipress/{magazine}/Books  or, using the snap, in\n ~/snap/rpipress-downloader/current/rpipress/{magazine} ~/snap/rpipress-downloader/current/rpipress/{magazine}/Books.  Note that the script conveniently let you know the path by printing an hyperlink in the console,\n$ rpipress-downloader -m magpi Latest MagPi issue is N°95 You are up to date Your favorite magazines are waiting for you in file:///home/artivis/snap/rpipress-downloader/5/rpipress  Please refer to the rpipress-downloader readme page for further information.\nHave a good reading!\n","date":1594252800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1594479504,"objectID":"9e002b88a15bd0c002572e16d17175da","permalink":"https://artivis.github.io/post/2020/rpipress/","publishdate":"2020-07-09T00:00:00Z","relpermalink":"/post/2020/rpipress/","section":"post","summary":"In this post we will discover the great magazines edited under the Raspberry Pi Press umbrella and discuss how to easily access them all.\n Content   The Raspberry Pi Press magazines  Bookshelf  rpipress-downloader   The Raspberry Pi Press magazines The Raspberry Pi Press is a part of the Raspberry Pi Foundation and the publisher of a great deal of magazines and books.","tags":["tutorial","Raspberry Pi","press","free magazines"],"title":"Downloading Raspberry Pi Press issues","type":"post"},{"authors":null,"categories":null,"content":"The 5th of June 2020 marks the release of ROS 2 Foxy Fitzroy, a 3 years long-term support (LTS) release and the first ROS 2 distribution to target Ubuntu 20.04.\nAs summarized in the ROS Discourse post, Foxy comes loaded with performance improvements, new features, and maybe most importantly, with tutorials! Get a full tour of the novelties by heading down to the Foxy release page.\nNow, you may be very excited about ROS 2 Foxy but you, just as I, haven\u0026rsquo;t moved to Ubuntu 20.04 just yet. But that will not stop us from getting our hands on all the goodies this new release offers.\nIndeed, in this post we will see how to install ROS 2 Foxy Fitzroy in a LXD container so that we can develop against the latest ROS 2 release without the need to upgrade our computer just yet.\nHereafter we will assume that your are familiar with the command terminal and that LXD is already installed on your machine. If you are new to LXD or looking to improve your ROS development with it, have a look to this post \u0026lsquo;ROS Noetic development workflow in LXC\u0026rsquo;.\nAlright let us get started.\n Content   Spawning an Ubuntu 20.04 LXD container  Installing ROS 2 Foxy Fitzroy  Quick test   Spawning an Ubuntu 20.04 LXD container So the first thing we have to do is to create a LXD container based on an Ubuntu 20.04 image. To do so we issue the following command,\nlxc launch ubuntu:20.04 ros2-foxy  It creates and starts a container named \u0026lsquo;ros2-foxy\u0026rsquo; based on an Ubuntu 20.04 image. So far so good.\nNow to start a shell in our fresh container we will type,\nlxc exec ros-foxy -- sudo --login --user ubuntu  We are now inside our container, logged as the non-root user \u0026lsquo;ubuntu\u0026rsquo;. Note that if the last command looks a bit unfriendly to you, you can make it a \u0026lsquo;lxc\u0026rsquo; alias (learn more about it in the aforementioned LXD post).\nNow that our container is up and running, we shall install Foxy.\nInstalling ROS 2 Foxy Fitzroy Inside our container, we will first add the ROS packages repository to our sources. Starting with the key,\n$ sudo apt-key adv --fetch-keys https://raw.githubusercontent.com/ros/rosdistro/master/ros.asc  then the repository,\n$ sudo apt-add-repository http://packages.ros.org/ros2/ubuntu  In case of trouble, you can also refer to the official installation guide.\nWe are all set to install Foxy!\nFor the installation, we can choose either of two options; we can choose to install only the base components, e.g. the communication libraries, message packages, command line tools, etc\u0026hellip;\n$ sudo apt install ros-foxy-ros-base  or we can install the base + RViz, demos and tutorials,\n$ sudo apt install ros-foxy-desktop  You can pick any depending on your needs. If you are not sure which to pick, I would recommend you install the desktop version in order to have all the tools you may need already installed. However, note that if you intend to use some graphical applications in your container, you have to take an extra step and set up some parameters for your container. All of this is detailed in the aforementioned LXD post.\nSince the container was created especially for Foxy, we will automatically source it in our \u0026lsquo;.bashrc\u0026rsquo;,\n$ echo \u0026quot;source /opt/ros/foxy/setup.bash\u0026quot; \u0026gt;\u0026gt; ~/.bashrc  Every times we will log into our container, ROS 2 Foxy will be sourced and we will be ready to develop!\nAt last, we can install the Python package \u0026lsquo;argcomplete\u0026rsquo; to enable autocompletion for the ROS 2 command line tools. This is totally optional, but also totally recommended:\nsudo apt install python3-argcomplete  With Foxy installed, all there is left to do is to take it for a spin.\nQuick test We will try the simple talker-listener demo, mixing cpp and Python to make sure that the installation went fine and that we can start developing right away. Note that if you installed the \u0026lsquo;base\u0026rsquo; version in the previous section, you will need to install the following packages,\nsudo apt install ros-foxy-demo-nodes-cpp ros-foxy-demo-nodes-py  For the test, let us start fresh and close our current shell. We will then open 2 new ones, one for the publisher and one for the subscriber.\nTo start the publisher in the first shell enter,\n$ ros2 run demo_nodes_cpp talker [INFO] [1591461939.792327469] [talker]: Publishing: 'Hello World: 1' [INFO] [1591461940.792228229] [talker]: Publishing: 'Hello World: 2' [INFO] [1591461941.792184798] [talker]: Publishing: 'Hello World: 3' ...  and we can see that it starts publishing messages right away.\nTo start the listener in the second shell enter,\n$ ros2 run demo_nodes_py listener [INFO] [1591461964.793113956] [listener]: I heard: [Hello World: 5] [INFO] [1591461965.792782570] [listener]: I heard: [Hello World: 6] [INFO] [1591461966.792823099] [listener]: I heard: [Hello World: 7] ...  and we can see that it receives messages right away as well.\nWe are all set!\n","date":1591401600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1591632633,"objectID":"9d67f85c814c83f333e6aed72f528a53","permalink":"https://artivis.github.io/post/2020/ros-foxy-install/","publishdate":"2020-06-06T00:00:00Z","relpermalink":"/post/2020/ros-foxy-install/","section":"post","summary":"The 5th of June 2020 marks the release of ROS 2 Foxy Fitzroy, a 3 years long-term support (LTS) release and the first ROS 2 distribution to target Ubuntu 20.04.","tags":["tutorial","ROS 2","Foxy","LXC","LXD"],"title":"Get started with ROS 2 Foxy today with LXD","type":"post"},{"authors":null,"categories":null,"content":"In this post, we will see how we can easily manage our personal configuration files - a.k.a. dotfiles. Yeah dotfiles, named after there common ~/.my_config form, you know, all of those small configuration files lying across our $HOME.\n Because there is no place like $HOME\n Because we are spending so much time on our machine, be it for work or for fun (both at the same time if you are lucky), we love to tweak our environment to our taste and needs. Change the UX, create some aliases, use some dark theme and what not, most if not all of these are saved in some configuration files somewhere. And since we spent so much time making a home for ourselves, wouldn\u0026rsquo;t it be great if we could quickly set it up again on a different computer? Change the house but keep the furniture and decorations? This is precisely what we are going to set up here.\n Content   Picking a dotfiles manager  Building our castle  Quickly setting up a new machine  Disposable tiny home   Picking a dotfiles manager Looking on the web for a dotfiles manager, you may find many of them - see a whole list of them here. Most of them work off the same principles, being a small set of utils to help manage our dotfiles. Management includes most importantly versioning, often through git and the installation of the files to their correct location as they are more than often expected to be found at a given path. You may want to give a look at the aforementioned list of managers and pick one that best answers your needs and expectations. Note that many are interchangeable.\nIn this post we settled using homeshick. There are two main reasons for this choice. Firstly, it is entirely written in bash, making it usable virtually anywhere. Secondly, it \u0026lsquo;installs\u0026rsquo; dotfiles on our system using symlinks rather than hard copies. The files thus exist in a single place. Some other nice features includes, being git-based, being cli-based, supporting multi dotfiles repos. It has to be noted tho that the project is not in a really active development and not very feature rich compared to other solutions. It is a thin-layer that does the job.\nAlright so how do we get started?\nBuilding our castle homeshick relies around the concept of castles which are nothing more than git repositories. A castle contains all of our dotfiles which are organized with the same layout as our home directory. But before building our castle, we need to install the appropriate tool. To install homeshick, nothing easier, we simply clone its repository in our home:\n$ git clone https://github.com/andsens/homeshick.git $HOME/.homesick/repos/homeshick  And we are done. Now to use it, we only have to source it, e.g. directly in our .bashrc,\n$ echo \u0026quot;source ~/.homesick/repos/homeshick/homeshick.sh\u0026quot; \u0026gt;\u0026gt; ~/.bashrc  We can also source its tab completion tool to ease our life,\n$ echo \u0026quot;source ~/.homesick/repos/homeshick/completions/homeshick-completion.bash\u0026quot; \u0026gt;\u0026gt; ~/.bashrc  Alright, we are done with the installation, let us start creating the said castle.\nFirst we create a new local git repo through homeshick cli tool,\n$ homeshick generate dotfiles  This creates an empty castle named \u0026lsquo;dotfiles\u0026rsquo; in ~/.homesick/repos/dotfiles/. To populate our castle with a dotfile, we make use of the \u0026lsquo;track\u0026rsquo; command:\n$ homeshick track {castle} {dotfile}  To track our first file, say e.g. .bashrc, we simply issue,\n$ homeshick track dotfiles ~/.bashrc  The command copies the file in our castle at ~/.homesick/repos/dotfiles/home/.bashrc and replaces the original file with a symlink to the copy.\nNow all we have to do is to commit our change and save our castle online,\nTo move to our local repository, we enter,\n$ homeshick cd dotfiles  and we can now use the usual git commands,\n$ git add . $ git commit -m 'add .bashrc'  Let us save our castle online, e.g. on GitHub,\n$ git remote add origin git@github.com:user/dotfiles.git $ git push -u origin master  We may now repeat this operation for each and every configuration file we would like to save. With our castle safely backed up online, we will now see how we can quickly set up our environment on a new machine.\nQuickly setting up a new machine Whether you bought a new computer or nuked your old hardware with a fresh new distro, you will now witness the true power of homeshick.\nTo install our cosy environment on a fresh distro, all we have to do is,\n Install homeshick $ git clone https://github.com/andsens/homeshick.git $HOME/.homesick/repos/homeshick $ source ~/.homesick/repos/homeshick/homeshick.sh   Import our castle $ homeshick clone git@github.com:user/dotfiles.git   Let homeshick works its magic $ homeshick link dotfiles    Voila! Home sweet home.\nOf course this post is only a quick overview of a given dotfiles manager. I won\u0026rsquo;t detail here all of its options and features and let you discover them for yourself in its wiki. As mentioned previously many dotfiles managers rely on a git repository and the same layout as homeshick so you can get started with it and later move to another one which better fits your needs.\nAt this point you may be wondering if this is really worth it given that you probably install a fresh distro every 2 years or so and completely change hardware even less frequently. Well, fellow developer, aren\u0026rsquo;t you using containers? If not, you definitely should consider it and check this other post where I detail a development workflow for ROS in LXD.\nDisposable tiny home If you are like me, trying your best to keep a tidy laptop while messing around with plenty of different software toys, then you may have had one of these days during which you spawn several containers. Containers in which we don\u0026rsquo;t have our sweet bash aliases; on our very own machine! But thanks to homeshick we can now start up a fresh container and have it mimic $HOME in a matter of seconds! Let me demonstrate it for you with a LXD container,\n$ lxc launch ubuntu:20.04 tmp-20-04 $ lxc profile add castle tmp-20-04 $ lxc ubuntu tmp-20-04  Ahhh, what a cozy tiny disposable home!\nThat seemed too easy to you? Alright I confess, I used some of my own aliases here. But isn\u0026rsquo;t it what this whole post is about? Note that the above 3 lines really boils down to,\n$ lxc launch ubuntu:20.04 tmp-20-04 $ lxc exec tmp-20-04 -- sudo --login --user ubuntu ... $ git clone https://github.com/andsens/homeshick.git $HOME/.homesick/repos/homeshick $ source ~/.homesick/repos/homeshick/homeshick.sh $ homeshick clone git@github.com:user/dotfiles.git $ homeshick link dotfiles  With this example, I hope that I managed to offer you a glimpse at the power of homeshick (and more generally of dotfiles managers), especially when coupled to a containerized workflow.\nBefore closing this post, let me give you one last tip. Because we made our containerized workflow rather seamless with our host, it can be easy to lose track of which shell is in a container and which is not. To differentiate them, add the following to your .bashrc:\nfunction prompt_lxc_header() { if [ -e /dev/lxd/sock ]; then echo \u0026quot;[LXC] \u0026quot;; fi } PS1='$(prompt_lxc_header)'$PS1  When used in a container, a shell prompt in the said container will now look something like:\n[LXC] ubuntu@tmp-20-04:~$  No more confusion 👍\n","date":1590969600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1591463237,"objectID":"e2a84bd28e0b382911532f574e4380e4","permalink":"https://artivis.github.io/post/2020/dotfiles/","publishdate":"2020-06-01T00:00:00Z","relpermalink":"/post/2020/dotfiles/","section":"post","summary":"In this post, we will see how we can easily manage our personal configuration files - a.k.a. dotfiles. Yeah dotfiles, named after there common ~/.my_config form, you know, all of those small configuration files lying across our $HOME.","tags":["tutorial","dotfiles","HOME","homeshick","LXC","LXD"],"title":"Managing dotfiles","type":"post"},{"authors":null,"categories":null,"content":"Here we are, looking for online visibility. How does one set that up quickly when starting from scratch? Do you Remember those HTML courses?\nYeah me neither.\nBut fortunately for us it is now easier than ever!\nWe will discuss in this post how to create our own website with the Hugo framework from a template and how to deploy it to GitHub. The prerequisites are,\n GitHub markdown  LXD (optional)  You may find GitHub tutorials here and there. Pages of our website will be written in markdown. You can learn more about markdown from those tutorials, in English and in French. And if you only need a brief refresh, here is the syntax supported by our website. Finally, you can find a LXD tutorial here.\nNow, let us set up the necessary stuff to get started, shall we?\nPicking a website template To build our website, we will use the framework Hugo. It is very convenient for our use case because it comes with a ton of predefined website themes and it is very simple to use.\nFor the purpose of this tutorial we will use the very theme of this website, namely, Academic. This theme is rather clean, well organized, fairly simple to use and most importantly it is well documented! Furthermore, it can be found pre-bundled in a Hugo project so that it is pretty much clone and play. However, at the time of writing, this theme requires Hugo *Extended* version 0.67+. This distinction is important because, while it is conveniently packaged as a snap, the snap only offers the classic version, not the Extended. Therefore we have to fetch its debian package and install it manually.\nFirst, let us clone the ready-to-go Academic bundle on our machine:\ncd ~/ git clone https://github.com/sourcethemes/academic-kickstart.git my_website cd my_website git submodule update --init --recursive  Prepping the tools To avoid polluting our system, we will set up a Linux container in which we will install Hugo Extended. The container is totally optional and you can do the installation directly on your machine. If you do not wish to use a container, skip directly to Hugo installation\nSetting up the LXC Let us start a fresh and pull a new Ubuntu 18.04 instance,\nlxc launch ubuntu:18.04 hugo  We will now mount a disk device to share the website source code between our machine and the container:\nlxc config device add hugo workspace disk source=~/my_website path=/home/ubuntu/my_website lxc config set hugo raw.idmap \u0026quot;both $(id -u) $(id -g)\u0026quot; lxc restart hugo  The default installation of LXD set up a bridged network so that containers live behind a NAT on the host. Therefore, we have to forward the port on which our website is served by the Hugo framework. To do so, issue the following command:\nlxc config device add hugo proxy1313 proxy connect=tcp:127.0.0.1:1313 listen=tcp:0.0.0.0:1313  The container is all set up. We can log to it with:\nlxc exec hugo -- su --login ubuntu  Installing Hugo We will download the Hugo extended debian directly from it GitHub repository. To do so, enter in the terminal:\nwget https://github.com/gohugoio/hugo/releases/download/v0.70.0/hugo_extended_0.70.0_Linux-64bit.deb  At the time of writing, the latest Hugo Extended release is version 0.70.0.\nWe can now install it with:\nsudo dpkg -i hugo_extended_*.deb  First view of our website Let the show begin. We are now ready to spawn our website and browse it. In a terminal, enter:\ncd ~/my_website hugo server  Voila!\nThe website it up and running! To visualize it, open your web browser at the address http://localhost:1313. That was easy right?\nMaking the website your own We have a great template up and running, it is now time to make it our own. The Academic theme comes with a ton of options and configurations allowing us to truly personalize it to our liking and use case. And since its online documentation is so great, I will let you discovers by yourself all the possibilities the theme offers. Head down to the Academic get started documentation and have fun!\nJust a quick advice, as you edit your website, let Hugo run. It is able to update the website live so that you see your changes take effect immediately in your web browser!\nDeploying the website to GitHub Once our website is ready to be made public, all there is to do is to push it to GitHub. Well, almost.\nIn your GitHub account, we will create a repository to host your website. To do so hit the tiny cross (+) in the top-right of GitHub and select new repository. For GitHub to be able to figure out that this particular repository is your personal website we need to give it a specific name in the form : \u0026lt;your-github-user-name\u0026gt;.github.io.\nWe will now prepare to push the website to this repository.\nFirst we will add the GitHub repository we just created as our remote,\ngit add remote origin https://github.com/\u0026lt;your-github-user-name\u0026gt;/\u0026lt;your-github-user-name\u0026gt;.github.io.git  and change our branch name to avoid later mess,\ngit branch -m master builder  Here comes the final step before pushing to GitHub. We must build our website, or rather let Hugo do it for us. Indeed so far we have edited the template that Hugo uses to build the website. We have visualized it in our browser but the template cannot be deployed directly to GitHub, it must be built. To build it locally, nothing easier, simply run:\nhugo  You will notice a new folder named public in our project. It contains the generated website. It is this content that we must push to our repository. Furthermore, it must be pushed specifically to the master branch. That\u0026rsquo;s a limitation of personal website on GitHub.\nAutomatic deployment So how could we automatize this build and deploy process?\nWe will add a small script so that every times we push some new content on the builder branch, GitHub will take care of calling Hugo (building) and moving the public folder directly on the master branch (deploying).\nFor that, we will use GitHub actions and more specifically the actions-hugo. Sorry buddy but I\u0026rsquo;ll skip the details about actons here as it is all new to me as well. That could be the topic for a later post tho.\nWe will simply create a new file in our project to configure the action:\ncd ~/my_website touch .github/workflows/deploy-website.yml  which we will edit as follows:\nname: deploy website # We will run the actions whenever something # is pushed to the branch 'builder' on: push: branches: - builder jobs: # Our action is called 'deploy' and runs on Ubuntu 18.04 deploy: runs-on: ubuntu-18.04 # The action executes the following steps steps: # It fetch our repository and its submodules - uses: actions/checkout@v2 with: submodules: true # Fetch Hugo themes fetch-depth: 0 # Fetch all history for .GitInfo and .Lastmod # It then set up Hugo Extended - name: Setup Hugo uses: peaceiris/actions-hugo@v2 with: hugo-version: '0.68.3' extended: true # It runs Hugo to generate the website - name: Build run: hugo --minify # It copies the content of the 'public' folder to the branch 'master' - name: Deploy uses: peaceiris/actions-gh-pages@v3 with: github_token: ${{ secrets.GITHUB_TOKEN }} publish_dir: ./public publish_branch: master  With our automatic deployment configured, all there remains to do is to push to GitHub!\nLet us remove the \u0026lsquo;public\u0026rsquo; folder is it exists,\ncd ~/my_website rm -r public  and commit all of our changes,\ngit add . git commit 'made the website my own'  Finally, we push the changes upstream,\ngit push origin builder  Voila!\nAfter a couple minutes your website is now available at the address:\nhttps://\u0026lt;your-github-user-name\u0026gt;.github.io/\nCongrats on your new online visibility, our job here is done.\nBonus: Academic publications If you happen to have some academic publications that you would like to showcase on your website, we will install a Python tool called academic that will help us to automatically generate pages from Bibtex.\nFirst we will install pip3:\napt install python3-pip  to then install academic:\npip3 install -U academic  Given that we have a .bib file that contains all of our publications, we can generate the pages as follows:\ncd ~/my_website academic import --bibtex \u0026lt;path_to_your/publications.bib\u0026gt;  You can find more information in the Academic theme documentation.\n","date":1588982400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1590795601,"objectID":"1935ac9469e8a1f3c15b0c1795f0e181","permalink":"https://artivis.github.io/post/2020/my-website/","publishdate":"2020-05-09T00:00:00Z","relpermalink":"/post/2020/my-website/","section":"post","summary":"Here we are, looking for online visibility. How does one set that up quickly when starting from scratch? Do you Remember those HTML courses?\nYeah me neither.\nBut fortunately for us it is now easier than ever!","tags":["tutorial","website","github","hugo","academic"],"title":"My website","type":"post"},{"authors":null,"categories":null,"content":"In this post, we will discuss how to setup a Linux container - a.k.a LXC - for our ROS Noetic development. Developing in containers has several advantages such as:\n allowing us to use a different Linux distribution than the one we\u0026rsquo;ve installed on our host machine providing a repeatable course of actions messing around, installing a tons of dependencies without polluting our computer burning the container to the ground and starting fresh again easily  There are of course many other upsides but those are the one we are really interested in for now. We will see first how to get started with LXC and install the latest ROS release Noetic. We will then configure our container so that it is able to share a workspace with our host machine. We will also enable the use of graphical applications from the container (e.g. Rviz, Gazebo).\nThe main prerequisites for this post are to be familiar with:\n the command terminal in Linux ROS development LXC  Note that I will be linking resources throughout the text, make sure to check them whenever you need further information.\nFinally, while we will be focusing on the latest ROS Noetic release, the setup presented here applies not only to other ROS distributions but likely to most projects, be them ROS-based or not.\n Content   Setting up the LXC   LXC aliases to the rescue    Install ROS Noetic  Mounting a local workspace  Using graphical applications   Creating a LXD profile  Dedicated graphic card    Profile all the things!  Wrapping up   Setting up the LXC We will start by installing LXD, a lightweight container hypervisor which extends LXC functionality over the network. LXD uses LXC under the covers for some container management tasks and provides the \u0026lsquo;lxc\u0026rsquo; command line interface tool we will use throughout this post. For more information, you can refer to the LXC and LXD documentation on the Ubuntu website.\nAlright, let us install LXD as a snap to make sure we always run the most up to date stable version:\n$ sudo snap install lxd  Before we can create our first container, we must initialize LXD,\n$ sudo lxd init  This command will prompt you with a bunch of questions to fine tune LXD use. Unless you know what you are doing, you can safely hit the default answers.\nFinally, we will add our user to the \u0026lsquo;lxd\u0026rsquo; group so that we can run lxd commands without sudo,\n$ sudo gpasswd -a \u0026quot;${USER}\u0026quot; lxd  You should log out and log in again for this to take effect.\nCreating the container To create a new container, we will use the following command,\n$ lxc launch {remote}:{image} {container-name}  Since Noetic runs on Ubuntu 20.04, we will fetch a Ubuntu 20.04 image from the official Ubuntu remote,\n$ lxc launch ubuntu:20.04 ros-noetic  We can check that the container was properly created and launched,\n$ lxc list +---------------+---------+-----------------------+-----------------------------------------------+-----------+-----------+ | NAME | STATE | IPV4 | IPV6 | TYPE | SNAPSHOTS | +---------------+---------+-----------------------+-----------------------------------------------+-----------+-----------+ | ros-noetic | RUNNING | 10.160.218.172 (eth0) | dd42:5ke1:fr68:2ca4:236:eff3:fe3r:7c21 (eth0) | CONTAINER | 0 | +---------------+---------+-----------------------+-----------------------------------------------+-----------+-----------+  With our container up and running, we can open a shell in it with a non-root user with the following command,\n$ lxc exec ros-noetic -- sudo --login --user ubuntu  I know, this command is not very pretty nor easy to remember. But worry not, we will create an alias to ease future use.\nLXC aliases to the rescue LXC aliases, just like bash aliases, allow use to create a new CLI keywords to which we can associate an action. The command to create a new alias is,\n$ lxc alias add {alias} '{command}'  As an example, let us create a shorter version of the lxc list command that also prints a more compact result:\n$ lxc alias add ls 'list --format csv -c n'  We can check that the alias is correctly created,\n$ lxc alias list +--------+----------------------------------------------------------------------------------+ | ALIAS | TARGET | +--------+----------------------------------------------------------------------------------+ | ls | list --format csv -c n | +--------+----------------------------------------------------------------------------------+  And we can now simply use it,\n$ lxc ls ros-noetic  That\u0026rsquo;s pretty neat.\nBut our main goal with aliases was to simplify our shell login to the container, so let\u0026rsquo;s just do that. Borrowing from the excellent blog post by Simos Xenitellis about LXC aliases, we will create a new alias \u0026lsquo;ubuntu\u0026rsquo; such as,\n$ lxc alias add ubuntu 'exec @ARGS@ --mode interactive -- /bin/sh -xac $@ubuntu - exec /bin/login -p -f '  This alias allows us now to simply connect to our container with,\n$ lxc ubuntu ros-noetic  That\u0026rsquo;s much better isn\u0026rsquo;t it?\nInstall ROS Noetic  ROS Noetic is the latest and final ROS 1 release. The ROS project hasn\u0026rsquo;t come to an end, on the contrary, it rather look forward and focus its efforts toward the second version, namely ROS 2. Nevertheless, ROS Noetic is an important release because it targets Ubuntu 20.04, has official Python 3 support and will be supported until May 2025 (more information on Noetic wiki page). That leaves us plenty of time to learn and move to ROS 2.\nTo install it, let\u0026rsquo;s first connect to our container using our new LXC alias,\n$ lxc ubuntu ros-noetic  First, we will add the ROS packages repository to our sources. Starting with the key,\n$ apt-key adv --fetch-keys https://raw.githubusercontent.com/ros/rosdistro/master/ros.asc  then the repository,\n$ apt-add-repository http://packages.ros.org/ros/ubuntu  In case of trouble, you can also refer to the official documentation.\nWe are all set to install ROS Noetic!\nHere we can choose either of three installations; we can install only the core components,\n$ sudo apt install ros-noetic-ros-base  or core + the visualization stack (e.g. Rviz),\n$ sudo apt install ros-noetic-desktop  or core + the visualization + simulation stacks (e.g. Gazebo),\n$ sudo apt install ros-noetic-desktop-full  You can pick any depending on your needs. If you are not sure, I would recommend you install only the core components and later install other packages on a per-need basis:\n$ sudo apt install ros-noetic-ros-base ... $ sudo apt install ros-noetic-\u0026lt;package-I-need\u0026gt;  Simply to keep the size of the container as small as possible.\nFinally, we will automatically source Noetic since this container is dedicated to it,\n$ echo \u0026quot;source /opt/ros/noetic/setup.bash\u0026quot; \u0026gt;\u0026gt; ~/.bashrc  Every times we will log into our container, ROS Noetic will be sourced and we will be ready to develop!\nMounting a local workspace What would our development workflow look like without some actual source code to work on? Well, let us set up our ROS workspace.\nRather than copying/creating our workspace in the container, we will keep it on the host machine. By doing so, not only the workspace will survive deleting the LXC (persistence) but we will also be able to share it across several LXC thus across several ROS distros.\nHereafter, we will assume our workspace to be simply ~/workspace on the host with the classic tree,\n$ tree ~/workspace /home/user/workspace/ └── src └── my_ros_package └...  To share a folder with the container, we have to add a \u0026lsquo;device disk\u0026rsquo; to it. The general command to do so is of the form,\n$ lxc config device add {container} {device-name} disk source={full-path-to-folder} path={full-path-inside-container}  filling up the placeholders for our use case, it reads,\n$ lxc config device add ros-noetic workspace disk source=~/workspace path=/home/ubuntu/workspace  Once the device added, we have to configure the access rights so that we can read and write the folder and its content in the container,\n$ lxc config set ros-noetic raw.idmap \u0026quot;both $(id -u) $(id -g)\u0026quot;  We now have to restart the container for the changes to take effects,\n$ lxc restart ros-noetic  Let us log back into our container,\n$ lxc ubuntu ros-noetic  and verify that the folder is properly mounted,\n$ ls -l drwxr-xr-x 22 ubuntu ubuntu 4096 May 22 21:21 workspace  Looks like we are good!\nIn this section we have configured our container through the lxc config cli tool. Note that container configuration is saved in a yaml file, which you can review with,\n$ lxc config show {container}  and directly edit with,\n$ lxc config edit {container}  More on that later.\nUsing graphical applications This part is totally optional and depends on whether you are planning to run some graphical applications (e.g. Rviz, Gazebo) in your container or not. If you are not interested in running any gui in your container, you may still want to have a quick look before jumping at the \u0026lsquo; Profile all the things!\u0026rsquo; section. If you do want to run graphical applications, then we have to configure the container to support that.\nUnlike in the previous section, we are not going to use the lxc config tool to configure our container. Instead, we will introduce lxc profile as a way to create easily reusable configurations.A profile is a set of parameters that can be applied to a container in one go. It can describe a full fledged setup or a particular feature as in our case below. Furthermore a profile can be use by a single container or many. Reusability!\nCreating a LXD profile Let us first create a profile named gui,\n$ lxc profile create gui  we can now edit the profile,\n$ lxc profile edit gui  and paste the following,\nconfig: environment.DISPLAY: :0 raw.idmap: both 1000 1000 description: Enables graphical apps use. devices: X0: path: /tmp/.X11-unix/X0 source: /tmp/.X11-unix/X0 type: disk mygpu: type: gpu name: gui used_by: []  Alternatively, you can use the following one liner,\ncurl https://gist.githubusercontent.com/artivis/37c961e157e99f6fcaff0204a0f59731/raw/ca4abd1a3c6b1d8a74910207903ac7723685dce1/gui.yaml | lxc profile edit gui  In this profile, there might be a couple things for you to tweak depending on your machine. For instance your user id and guid,\nraw.idmap: both 1000 1000  which you can retrieve respectively with:\n$ id -u 1000 $ id -g 1000  You may also have to check your graphic card in use looking at the directory /tmp/.X11-unix/.\nNow that our profile is set up, we have to add it to our container,\nlxc profile add ros-noetic gui  As previously, we have to restart the container for those change to take effect,\nlxc restart ros-noetic  Alright, let us try to open Rviz to make sure everything went fine. Open two shells to the container, one running the roscore and the second running Rviz:\nShell 1\n$ roscore  Shell 2\n$ rosrun rviz rviz  We are getting really close to our regular development experience aren\u0026rsquo;t we?\nDedicated graphic card If you have a dedicated graphic card on your host machine, you will also have to install the very same driver in the container in order to use graphical applications. If you have an Nvidia card, the following should help you. To figure out the driver version on the host we\u0026rsquo;ll type,\n$ nvidia-smi Mon May 12 11:59:59 2020 +-----------------------------------------------------------------------------+ | NVIDIA-SMI 440.82 Driver Version: 440.82 CUDA Version: 10.2 | +-------------------------------+----------------------+----------------------+  All we have to do now is to install the same driver in the container,\n$ sudo apt install nvidia-440  Profile all the things We have seen in section \u0026lsquo;Creating a LXD profile\u0026rsquo; how to create a LXC profile to easily support running graphical apps in our container(s). As we mentioned before, a profile really only is a set of configurations for our container. So one may ask\n can\u0026rsquo;t we create some other profiles to further group all the configs we\u0026rsquo;ve seen?\n Well, yes we can! And guess what? Containers can have several profiles! So we could totally create another profile to automatically add the ROS apt repository, both for ROS 1 and ROS 2 respectively:\n$ lxc profile create ros-apt $ lxc profile edit ros-apt  and add,\nconfig: user.user-data: | #cloud-config runcmd: - \u0026quot;apt-key adv --fetch-keys https://raw.githubusercontent.com/ros/rosdistro/master/ros.asc\u0026quot; - \u0026quot;apt-add-repository http://packages.ros.org/ros/ubuntu\u0026quot; - \u0026quot;apt-add-repository http://packages.ros.org/ros2/ubuntu\u0026quot; description: \u0026quot;Add ROS apt repository\u0026quot; devices: {} name: ros used_by: {}  Similarly we could create another profile to easily share our ROS workspace as well,\n$ lxc profile create ros-ws $ lxc profile edit ros-ws  and add,\nconfig: raw.idmap: both 1000 1000 description: \u0026quot;Share the ROS workspace\u0026quot; devices: workspaces: path: /home/ubuntu/workspace source: /home/user/workspace type: disk name: ros-ws used_by: {}  And remember to tweak both profiles (user id/guid etc.).\nBoth profiles will greatly help when creating a new container. However, before we get all excited, let me tell you that we have to be cautious when using them. The reason is that both should be added a rather specific times of the container creation. Let us see when that is.\nFirst, the \u0026lsquo;ros-apt\u0026rsquo; profile makes use of cloud-init to preconfigure the container meaning that our apt-key/apt-add-repository command will be run only once when the container is first created (see this other blog post by Simos Xenitellis for more info about cloud-init in LXD). To create a container with given profile(s), the lxc launch commands changes to,\n$ lxc launch --profile {profile-a} --profile {profile-b} {remote}:{image} {container-name}  which in our case looks like,\n$ lxc launch --profile default --profile ros-apt ubuntu:20.04 ros-noetic2  Let me insist again. If you try to add the \u0026lsquo;ros-apt\u0026rsquo; profile after the container was created, nothing will happen: lxd profile add ros-noetic ros-apt!\nConcerning our \u0026lsquo;ros-ws\u0026rsquo; profile, it is a bit of the opposite situation. Indeed, when creating the container, a whole bunch of things are ran before the \u0026lsquo;ubuntu\u0026rsquo; user is set up. Since we are linking our workspace to /home/ubuntu/ we may arrive to early so to speak and it results in messing up the proper set up of the user. For this profile, we therefore have to add it after the container creation (lxc launch --profile ros-ws {remote}:{image} {container-name}).\nWe can add our ros-ws profile to a container with,\n$ lxc profile add ros-noetic ros-ws  This whole tempo story sounds annoying. Alright let\u0026rsquo;s call it a day and summarize how to set up a new container.\nWrapping up Well, that was quite a journey in LXD realm. But our efforts were not vain for we have learned a lot about LXD and set up some great tools.\nSoon, the ROS 2 Foxy distro will be released (5th of June). How will we then create a Foxy container? Well, that\u0026rsquo;s quite simple now:\n$ lxc launch --profile default --profile ros-apt --profile gui ubuntu:20.04 ros-foxy $ lxc profile add ros-foxy ros-ws $ lxc ubuntu ros-foxy ... $ sudo apt install ros-foxy-desktop  Off we go!\n","date":1588982400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1590847791,"objectID":"6e9ac8c4106f150dee9ba7aed739cb49","permalink":"https://artivis.github.io/post/2020/lxc/","publishdate":"2020-05-09T00:00:00Z","relpermalink":"/post/2020/lxc/","section":"post","summary":"In this post, we will discuss how to setup a Linux container - a.k.a LXC - for our ROS Noetic development. Developing in containers has several advantages such as:\n allowing us to use a different Linux distribution than the one we\u0026rsquo;ve installed on our host machine providing a repeatable course of actions messing around, installing a tons of dependencies without polluting our computer burning the container to the ground and starting fresh again easily  There are of course many other upsides but those are the one we are really interested in for now.","tags":["tutorial","LXC","LXD","ROS","Noetic","Ubuntu"],"title":"ROS Noetic development workflow in LXC","type":"post"},{"authors":null,"categories":null,"content":"In this post, we will see how to SSH a Raspberry Pi Zero over USB from a Ubuntu-based host. Moreover we will make sure the the Pi Zero has access to internet through the host so that we can install/update some software.\nContent   Install Raspbian  SSH over USB  Set USB Gadget mode  Set the connection to \u0026lsquo;Shared with other computers\u0026rsquo;  Set a static MAC address  Set a static IP address  Install Raspbian There exists plenty of tutorials on the topic of installing Raspbian (see the the official documentation) therefore I will not detail it here. To summarize, you have to,\n Download Raspian (Lite for headless) Burn the image on a micro SD card with Etcher  Before unplugging the card we will enable SSH connections. To do so, open the boot partition on the card and simply create an empty ssh file:\n$ cd /path/to/root/ $ touch ssh  Plug the card on the Pi Zero  SSH over USB With the micro SD card ready we can now plug the USB cable to our host and Pi Zero. However, note that while the Pi Zero has two micro USB port, only one supports USB On-The-Go (OTG). It is this feature that will allows us to treat the connection as an Ethernet connection. The port in question is the innermost one, the one closer to the center of the board, as shown in the image below.\nSet USB Gadget mode We would like to be able to access the Pi Zero through SSH from our machine using a USB cable. To do that we will have to edit two files.\nFirst, edit the file /boot/config.txt and append this line at the end:\ndtoverlay=dwc2  Second, we will edit the file /boot/cmdline.txt. After rootwait, we will add\nmodules-load=dwc2,g_ether  ⚠ pay attention to leave only one space between rootwait and the new text otherwise it might not be parsed correctly.\n⚠ Note that there might already be some text after rootwait in which case you still must add the following immediately after rootwait! Again, leave a single space after rootwait but also after g_ether.\nThe Pi Zero is fully configured, we can now configure our host.\nSet the connection to Shared with other computers On your Linux host, go to the network connections editor. In the \u0026lsquo;IPv4 Settings\u0026rsquo; tab, set \u0026lsquo;Method:\u0026rsquo; to \u0026lsquo;Shared with other computers'. Refresh the connection (dis/connect), after what you should be able to SSH to the Pi Zero.\nTo SSH to the Pi Zero, open a terminal on your host and type:\n$ ssh pi@rasberrypi.local  You will be prompted for a password, use the default one for user \u0026lsquo;pi\u0026rsquo;. And do not forget to change it down the line!\nWe are now connected on our board, let\u0026rsquo;s try internet out. Since we are using the method \u0026lsquo;Shared with other computers\u0026rsquo; we should be able to access internet:\n$ ping -c 3 www.google.com PING www.google.com (172.217.13.100) 56(84) bytes of data. 64 bytes from yul02s04-in-f4.1e100.net (172.217.13.100): icmp_seq=1 ttl=57 time=10.7 ms 64 bytes from yul02s04-in-f4.1e100.net (172.217.13.100): icmp_seq=2 ttl=57 time=9.60 ms 64 bytes from yul02s04-in-f4.1e100.net (172.217.13.100): icmp_seq=3 ttl=57 time=10.4 ms --- www.google.com ping statistics --- 3 packets transmitted, 3 received, 0% packet loss, time 2001ms rtt min/avg/max/mdev = 9.609/10.276/10.783/0.499 ms  We are live!\nNote: On some distro, the \u0026lsquo;Shared with other computers\u0026rsquo; is not available from the default settings. In this case fire up nm-connection-editor from a terminal instead of the network connections editor.\nSet a static MAC address Because we are connecting our Pi Zero as an Ethernet device through USB OTG, each time the connection is established the board is issued with a new random MAC address. This can be quickly annoying if we rely on the MAC address for, e.g. assigning a static IP to our board directly in our router configurations. This set up it totally optional.\nIn the boot partition, edit the file cmdline.txt and append at the end,\ng_ether.host_addr=aa:bb:cc:dd:ee:ff  where aa:bb:cc:dd:ee:ff will be the static MAC address. We can use for instance the last mac address assigned to the board. To retrieve it, run ifconfig on the host machine and look for a connection along the lines enp0s29xxxxx.\nSet a static IP address We can also choose to assign a static IP address to our board so that we don\u0026rsquo;t have to look for it every now and then. To set a static IP address, edit the file /etc/dhcpcd.conf as follows,\ninterface usb0 static ip_address=10.42.0.42 static routers=10.42.0.1  Where 10.42.0.42 is your desired static IP address.\nReboot and have fun!\n","date":1588982400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1591021493,"objectID":"cabbba69f5793d2ee22a280969a26a29","permalink":"https://artivis.github.io/post/2020/pi-zero/","publishdate":"2020-05-09T00:00:00Z","relpermalink":"/post/2020/pi-zero/","section":"post","summary":"In this post, we will see how to SSH a Raspberry Pi Zero over USB from a Ubuntu-based host. Moreover we will make sure the the Pi Zero has access to internet through the host so that we can install/update some software.","tags":["tutorial","Raspberry Pi","Raspberry Pi Zero","SSH","headless","Raspian"],"title":"SSH the Raspberry Pi Zero over USB","type":"post"},{"authors":["J. Deray","J. Solà"],"categories":null,"content":"","date":1577836800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1589060872,"objectID":"7fa3cd8f940f34b8f7a680163f4018be","permalink":"https://artivis.github.io/publication/deray-joss-20/","publishdate":"2020-05-09T22:12:35.024894Z","relpermalink":"/publication/deray-joss-20/","section":"publication","summary":"","tags":null,"title":"Manif: A micro Lie theory library for state estimation in robotics applications","type":"publication"},{"authors":["J. Deray","B. Magyar","J. Solà","J. Andrade-Cetto"],"categories":null,"content":"","date":1572566400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1594247803,"objectID":"c85af36e1eed8e87dcbf3bc0775a3489","permalink":"https://artivis.github.io/publication/deray-iros-19/","publishdate":"2020-05-09T22:12:35.024619Z","relpermalink":"/publication/deray-iros-19/","section":"publication","summary":"This paper proposes the use of piecewise C^n smooth curve for mobile-base motion planning and control, coined Timed-Elastic Smooth Curve (TESC) planner. Based on a Timed-Elastic Band, the problem is defined so that the trajectory lies on a spline in SE(2) with non-vanishing n-th derivatives at every point. Formulated as a multi-objective nonlinear optimization problem, it allows imposing soft constraints such as collision-avoidance, velocity, acceleration and jerk limits, and more. The planning process is realtime-capable allowing the robot to navigate in dynamic complex scenarios. The proposed method is compared against the state-of-the-art in various scenarios. Results show that trajectories generated by the TESC planner have smaller average acceleration and are more efficient in terms of total curvature and pseudo-kinetic energy while being produced with more consistency than state-of-the-art planners do.","tags":null,"title":"Timed-elastic smooth curve optimization for mobile-base motion planning","type":"publication"},{"authors":["B. Magyar","N. Tsiogkas","J. Deray","S. Pfeiffer","D. Lane"],"categories":null,"content":"","date":1569888000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1594247803,"objectID":"fb969b8e309ab70a3bcc5186994d3c71","permalink":"https://artivis.github.io/publication/magyar-ral-19/","publishdate":"2020-05-09T22:12:35.024339Z","relpermalink":"/publication/magyar-ral-19/","section":"publication","summary":"Motion planning is one of the main problems studied in the field of robotics. However, it is still challenging for the state-of-the-art methods to handle multiple conditions that allow better paths to be found. For example, considering joint limits, path smoothness and a mixture of Cartesian and joint-space constraints at the same time pose a significant challenge for many of them. This letter proposes to use timed-elastic bands for representing the manipulation motion planning problem, allowing to apply continuously optimized constraints to the problem during the search for a solution. Due to the nature of our method, it is highly extensible with new constraints or optimization objectives. The proposed approach is compared against state-of-the-art methods in various manipulation scenarios. The results show that it is more consistent and less variant, while performing in a comparable manner to that of the state of the art. This behavior allows the proposed method to set a lower-bound performance guarantee for other methods to build upon.","tags":null,"title":"Timed-Elastic Bands for Manipulation Motion Planning","type":"publication"},{"authors":["J. Deray","J. Solà","J. Andrade-Cetto"],"categories":null,"content":"","date":1567296000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1594247803,"objectID":"d45f264f27e2c46ce1db468c8ab920ef","permalink":"https://artivis.github.io/publication/deray-ecmr-19/","publishdate":"2020-05-09T22:12:35.024059Z","relpermalink":"/publication/deray-ecmr-19/","section":"publication","summary":"This paper describes a self-calibration procedure that jointly estimates the extrinsic parameters of an exteroceptive sensor able to observe ego-motion, and the intrinsic parameters of an odometry motion model, consisting of wheel radii and wheel separation. We use iterative nonlinear on-manifold optimization with a graphical representation of the state, and resort to an adaptation of the pre-integration theory, initially developed for the IMU motion sensor, to be applied to the differential drive motion model. For this, we describe the construction of a pre-integrated factor for the differential drive motion model, which includes the motion increment, its covariance, and a first-order approximation of its dependence with the calibration parameters. As the calibration parameters change at each solver iteration, this allows a posteriori factor correction without the need of re-integrating the motion data. We validate our proposal in simulations and on a real robot and show the convergence of the calibration towards the true values of the parameters. It is then tested online in simulation and is shown to accommodate to variations in the calibration parameters when the vehicle is subject to physical changes such as loading and unloading a freight.","tags":["Calibration;Robot sensing systems;Wheels;Kinematics;Jacobian matrices;Mobile robots"],"title":"Joint on-manifold self-calibration of odometry model and sensor extrinsics using pre-integration","type":"publication"},{"authors":null,"categories":null,"content":"We will find in a near future more and more robots in our environment, including public places (e.g. malls, museums, hospitals). This requires a strong robustness in any task to ensure the safety of both people and robots. If nowadays we can find pretty accurate sensors (e.g. laser range finder), they however are of little help in crowded places. Especially since humanoid robots are still a curiosity for the public and as such are often completely surrounded. This is why it is important to investigate sensors which provide as much information as possible for the lowest cost possible. Omnidirectional cameras are well suited for this task. Indeed as shown in this thesis, two fisheye cameras are enough to get a full 360° view of the robot environment. This permits in a first place the robot to localize itself and in a second place, any usual use of camera can be adapted such as navigation, face detection etc.\nYoutube video     Few Images ","date":1530144000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1589060872,"objectID":"824d9f18212c0aa7d4461c87a1cf1ac6","permalink":"https://artivis.github.io/project/mscv_thesis/","publishdate":"2018-06-28T00:00:00Z","relpermalink":"/project/mscv_thesis/","section":"project","summary":"We will find in a near future more and more robots in our environment, including public places (e.g. malls, museums, hospitals). This requires a strong robustness in any task to ensure the safety of both people and robots.","tags":null,"title":"MSc Thesis","type":"project"},{"authors":["Joan Solà","Jeremie Deray","Dinesh Atchuthan"],"categories":null,"content":"","date":1514764800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1589060872,"objectID":"000f76c98004a939852ee292393c6e91","permalink":"https://artivis.github.io/publication/sola-18-lie/","publishdate":"2020-05-09T22:12:35.023827Z","relpermalink":"/publication/sola-18-lie/","section":"publication","summary":"A Lie group is an old mathematical abstract object dating back to the XIX century, when mathematician Sophus Lie laid the foundations of the theory of continuous transformation groups. As it often happens, its usage has spread over diverse areas of science and technology many years later. In robotics, we are recently experiencing an important trend in its usage, at least in the fields of estimation, and particularly in motion estimation for navigation. Yet for a vast majority of roboticians, Lie groups are highly abstract constructions and therefore difficult to understand and to use. This may be due to the fact that most of the literature on Lie theory is written by and for mathematicians and physicists, who might be more used than us to the deep abstractions this theory deals with. In estimation for robotics it is often not necessary to exploit the full capacity of the theory, and therefore an effort of selection of materials is required. In this paper, we will walk through the most basic principles of the Lie theory, with the aim of conveying clear and useful ideas, and leave a significant corpus of the Lie theory behind. Even with this mutilation, the material included here has proven to be extremely useful in modern estimation algorithms for robotics, especially in the fields of SLAM, visual odometry, and the like. Alongside this micro Lie theory, we provide a chapter with a few application examples, and a vast reference of formulas for the major Lie groups used in robotics, including most jacobian matrices and the way to easily manipulate them. We also present a new C++ template-only library implementing all the functionality described here.","tags":null,"title":"A micro Lie theory for state estimation in robotics","type":"publication"},{"authors":["J. Deray","J. Solà","J. Andrade-Cetto"],"categories":null,"content":"","date":1498867200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1589060872,"objectID":"6db2d53f82fb7607daaa1272edfece84","permalink":"https://artivis.github.io/publication/deray-ral-17/","publishdate":"2020-05-09T22:12:35.023482Z","relpermalink":"/publication/deray-ral-17/","section":"publication","summary":"We address in this letter the problem of loop closure detection for laser-based simultaneous localization and mapping (SLAM) of very large areas. Consistent with the state of the art, the map is encoded as a graph of poses, and to cope with very large mapping capabilities, loop closures are asserted by comparing the features extracted from a query laser scan against a previously acquired corpus of scan features using a bag-of-words (BoW) scheme. Two contributions are here presented. First, to benefit from the graph topology, feature frequency scores in the BoW are computed not only for each individual scan but also from neighboring scans in the SLAM graph. This has the effect of enforcing neighbor relational information during document matching. Second, a weak geometric check that takes into account feature ordering and occlusions is introduced that substantially improves loop closure detection performance. The two contributions are evaluated both separately and jointly on four common SLAM datasets and are shown to improve the state-of-the-art performance both in terms of precision and recall in most of the cases. Moreover, our current implementation is designed to work at nearly frame rate, allowing loop closure query resolution at nearly 22 Hz for the best case scenario and 2 Hz for the worst case scenario.","tags":null,"title":"Word ordering and document adjacency for large loop closure detection in 2D laser maps","type":"publication"},{"authors":["D. Gurung","C. Jiang","J. Deray","D. Sidibé"],"categories":null,"content":"","date":1370044800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1589060872,"objectID":"a126fbe7e6fa32b887faf2964b8e5fae","permalink":"https://artivis.github.io/publication/gurung-hal-00903898/","publishdate":"2020-05-09T22:12:35.023088Z","relpermalink":"/publication/gurung-hal-00903898/","section":"publication","summary":"In this project we develop a system that uses low cost web cameras to recognise gestures and track 2D orientations of the hand. This report is organized as such. First in section 2 we introduce various methods we undertook for hand detection. This is the most important step in hand gesture recognition. Results of various skin detection algorithms are discussed in length. This is followed by region extraction step (section 3). In this section approaches like contours and convex hull to extract region of interest which is hand are discussed. In section 4 a method is describe to recognize the open hand gesture. Two additional gestures of palm and fist are implemented using Haar-like features. These are discussed in section 5. In section 6 Kalman filter is introduced which tracks the centroid of hand region. The report is concluded by discussing about various issues related with the embraced approach (section 9) and future recommendations to improve the system is pointed out (section 10).","tags":null,"title":"Hand Gestures Recognition and Tracking","type":"publication"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1589831268,"objectID":"8576ec274c98b3831668a172fa632d80","permalink":"https://artivis.github.io/about/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/about/","section":"","summary":"","tags":null,"title":"","type":"widget_page"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1589831268,"objectID":"6d99026b9e19e4fa43d5aadf147c7176","permalink":"https://artivis.github.io/contact/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/contact/","section":"","summary":"","tags":null,"title":"","type":"widget_page"}]